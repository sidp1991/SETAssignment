Mounting google drive
from google.colab import drive
drive.mount('/gdrive')
%cd /gdrive
Importing Libraries
# Import libraries 
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings("ignore")
import pandas.util.testing as tm
Loading Dataset(AirQuality.xlsx) from Google Drive
#reading the dataset file using pandas
Data = pd.read_excel('/gdrive/MyDrive/SET/AirQualityUCI.xlsx')
About Dataset
Data.head()
# give 1st 5 row command name attribute and variables
Data.tail()
# give last 5 rows of the dataset
Data.shape
#shows number of rows and columns in the dataset
Data.describe()
# Generate descriptive statistics
Data.info()
#information about dataset
# Data Preprocessing
Pre-processing refers to the transformations applied to your data before feeding it to the Machine Learning algorithm, which affects the accuracy of final outcome. In python, scikit-learn library has a pre-built functionality under sklearn.preprocessing.Pandas library also offer such capabilities. There are many more options for pre-processing which weâ€™ll explore.
# Dealing with missing data
# Checking and counting for missing data points for each column
Data.isnull().sum()
# Deleting missing values
#Remove missing values
DataCleaned=Data.dropna()
DataCleaned.isnull().sum()
# Finding out the mode value for CO(GT) column
ModeValueForColor=Data['CO(GT)'].mode()[0]
print('Mode value for CO(GT) column is: ',ModeValueForColor)
# Now we can see all columns have zero missing values
print(Data.isnull().sum())
print(Data.info())
Data['CO(GT)'].value_counts() 
 # returns object containing counts of unique values
Data.columns
#find columns name air quality is working as data frame
# Data Preprocessing
Data.dtypes
#datatype of each attribute
# FINDING -200 USING SIMPLE FOR LOOPS AND THEN REPLACED
l=[]
for i in range(len(Data.columns)):
    f=Data.columns[i]
    count=0
    for j in range(len(Data[f])):
        if Data[f][j]==-200:
            count+=1
    l.append((f,count))                     
print("Values from each column that needs to be replaced with avg \n ",l)      
num=Data._get_numeric_data()
num[num<0]=0
Data
Data['CO(GT)'].value_counts()
# i.e all -200 are replaced with 0
# Outlier in Dataset is just -200

#Creating dataset 
np.random.seed(10) 
data =Data["CO(GT)"]
print(data)
fig = plt.figure(figsize =(10, 7)) 

# Creating plot 
plt.boxplot(data) 

# show plot 
plt.show() 
# Correlation with other variables
Data.corr()
corrmat=Data.corr()
top_corr_feature=corrmat.index
plt.figure(figsize=(30,20))
# to plot heat map
g=sns.heatmap(Data[top_corr_feature].corr(),annot=True,cmap='viridis')
sns.pairplot(Data)
Data.plot(kind='scatter',x='C6H6(GT)',y='PT08.S5(O3)')
plt.show()
Data["T"].plot.hist(bins=50)
## Conclusion
**Highest positive corrleation can be seen among T,RH -C6H6 with .97,.92 etc.**
# Prediction
#features
feature=Data
feature=feature.drop('Date',axis=1)
feature=feature.drop('Time',axis=1)
feature=feature.drop('C6H6(GT)',axis=1)
feature.head()
#labels
label = Data['C6H6(GT)']
label.head()
#test and train split
X_train,X_test,y_train,y_test = train_test_split(feature,label,test_size=.3)
print(X_train.shape,y_train.shape)
print(X_test.shape,y_test.shape)
lr = LinearRegression()
lr.fit(X_train,y_train)
lr.score(X_test,y_test)
y_pred = lr.predict(X_test)
y_pred
# The coefficients
print('Coefficients: \n',lr.coef_)
# The mean squared error
print('Mean squared error: %.2f'
      % mean_squared_error(y_test, y_pred))
# The coefficient of determination: 1 is perfect prediction
print('Coefficient of determination: %.2f'
      % r2_score(y_test, y_pred))
# the r squared value
print('R squared value: %.2f'%r2_score(y_test, y_pred))
